# Setup and run docker containers for streaming video inference on camera using TPU

version: '2.0'
services:
  object-detector-python:
    image: $APP_NAME
    privileged: true
    network_mode: bridge
    links:
      - "inference-server"
    logging:
      driver: "json-file"
      options:
        max-file: "5"
        max-size: "100k"
    depends_on:
      - inference-server
      - ssdlite_mobilenet_object
    environment:
      - INFERENCE_HOST=inference-server
      - INFERENCE_PORT=8501
      - MODEL_PATH=/model/ssdlite-mobilenet-v2-tpu
      - MODEL_INPUT_SIZE=300x300
      - OBJECT_LIST_PATH=/model/objects.txt
      - DEBUG=y
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ssdlite_mobilenet_object:/model:ro
      - /tmp:/output
      - /var/run/statuscache:/var/run/statuscache
      - /var/run/dbus:/var/run/dbus:rw
    devices:
      - /dev/datacache0:/dev/datacache0:rw

  inference-server:
    image: axisecp/larod-inference-server:2.3.0-api.4.0pre5-armv7hf-ubuntu20.04
    privileged: true
    network_mode: bridge
    logging:
      driver: "json-file"
      options:
        max-file: "5"
        max-size: "100k"
    command: /usr/bin/larod-inference-server -p 8501 -j 4
    depends_on:
      - ssdlite_mobilenet_object
    expose:
      - 8501
    volumes:
      - ssdlite_mobilenet_object:/model:ro
      - /run/dbus/system_bus_socket:/run/dbus/system_bus_socket
      - /tmp:/tmp

  ssdlite_mobilenet_object:
    image: axisecp/acap-dl-models:ssdlite-mobilenet-v2
    restart: 'no'
    volumes:
      - ssdlite_mobilenet_object:/model

volumes:
  ssdlite_mobilenet_object: {}
